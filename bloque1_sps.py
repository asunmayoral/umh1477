# -*- coding: utf-8 -*-
# Funciones b√°sicas para el bloque 1. Distribuciones y CMTD
import numpy as np          # importamos numpy como np
import pandas as pd         # importamos pandas como pd
import math
import random

from scipy import stats, optimize
from scipy.special import gamma

# Cargamos m√≥dulos de an√°lisis gr√°ficos
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set_theme(style = 'whitegrid')
# %config InlineBackend.figure_format = 'retina'


#============================================================================================
# DISTRIBUCIONES DISCRETAS
#============================================================================================

def graficar_discreta(x, fx):
  """
  Funci√≥n para representar gr√°ficamente la funci√≥n de masa de probabilidad y la funci√≥n de distribuci√≥n de una variable discreta.

  Args:
    x: valores de la variable discreta
    fx: funci√≥n de masa de probabilidad para cada valor de x

  Returns:
    Gr√°ficos de la funci√≥n de masa de probabilidad y la funci√≥n de distribuci√≥n.
  """
  # posiciones en el gr√°fico de los valores d ela variable discreta
  pos = np.arange(len(x))
  # Funci√≥n de distribuci√≥n
  fdist = [sum(fx[:(l+1)]) for l in range(len(fx))]

  # Entorno gr√°fico
  fig, ax = plt.subplots(1, 2, figsize=(7, 4))
  # Pintamos los puntos con x y la funci√≥n de masa de probabilidad
  ax[0].plot(pos, fx, 'bo');
  # Dibujamos las l√≠neas verticales correspondientes con sus caractar√≠sticas
  ax[0].vlines(pos, 0, fx, colors='b', lw=5, alpha=0.5);
  # Ponemos un t√≠tulo
  ax[0].set_title('Funci√≥n de masa de probabilidad')
  # Ponemos etiquetas a los ejes x e y
  ax[0].set_xticks(pos, labels=x)
  ax[0].set_ylabel('Probabilidad')
  ax[0].set_xlabel('Espacio muestral')

  #### Funci√≥n de distribuci√≥n
  # Pintamos los puntos con x y la funci√≥n de distribuci√≥n
  ax[1].plot(pos, fdist, 'bo');
  # Dibujamos las l√≠neas verticales correspondientes con sus caractar√≠sticas
  ax[1].vlines(pos, 0, fdist, colors='b', lw=5, alpha=0.5);
  # Ponemos un t√≠tulo
  ax[1].set_title('Funci√≥n de distribuci√≥n')
  # Ponemos etiquetas a los ejes x e y
  ax[1].set_xticks(pos, labels=x)
  ax[1].set_ylabel('Probabilidad')
  ax[1].set_xlabel('Espacio muestral')
  plt.tight_layout()
#---------------------------------------------------------------------------------------
# Funci√≥n para obtener un dataframe con la funci√≥n de masa de probabilidad y la funci√≥n de distribuci√≥n de una variable discreta.
def distr_discreta(x, fx):
  """
  Funci√≥n para obtener un dataframe con la funci√≥n de masa de probabilidad y la funci√≥n de distribuci√≥n de una variable discreta.

  Args:
    x: valores de la varaible discreta
    fx: funci√≥n de masa de probabilidad

  Returns:
    pdDataFrame con los valores de la variable, la funci√≥n de masa de probabilidad y la funci√≥n de distribuci√≥n.
  """
  # posiciones en el gr√°fico de los valores d ela variable discreta
  pos = np.arange(len(x))
  # Funci√≥n de distribuci√≥n
  fdist = [sum(fx[:(l+1)]) for l in range(len(fx))]
  return(pd.DataFrame({"x": x, "fmp":fx, "fdist":fdist}))

#---------------------------------------------------------------------------------------
# Simular una m.a. de una distribuci√≥n discreta y devolver media y varianza
def simula_discreta(x, fx, n):
  """
  Funci√≥n para simular una m.a. de una distribuci√≥n discreta y devolver
  media y varianza de los datos simulados.
  Args:
    x: valores de la variable discreta
    fx: funci√≥n de masa de probabilidad

  Returns
    Lista con el valor medio y desviaci√≥n t√≠pica de la variable de inter√©s en el periodo n de simulaci√≥n
  """
  muestra = np.random.choice(x, size = n, replace = True, p = fx)
  resul = [round(muestra.mean(),2), round(muestra.var(),2)]
  return(resul)
#============================================================================================

# ESTIMACI√ìN MONTE CARLO
# Funci√≥n para obtener el estimador Monte Carlo de h(x) y un intervalo de confianza al 95%
def MC_estim(sims):
  """
  Funci√≥n para obtener el estimador Monte Carlo de h(x) y un intervalo de confianza al 95%

  Args:
   sims: Si queremos un estimador de h(x) pasamos directamente als simulaciones,
          mientras que si deseamos una probabildiad debemos pasar el vector 1-0
          que cumple con las condiciones de la probabilidad buscada

  Returns: 
    Devuelve el estimador e intervalo de confianza por Monte Carlo
  """
  from scipy.stats import norm

  # N√∫mero de simulaciones cargadas
  size = len(sims)
  # Estimador MC
  estim = sims.mean()
  # Estimador MC del IC
  error = math.sqrt(sims.var())*math.sqrt(size-1)/size
  cuantil = norm.ppf(1-0.05/2)
  ic_low = estim - cuantil*error
  ic_up = estim + cuantil*error
  # Resultado
  return([round(estim,4), round(ic_low,4), round(ic_up,4)])

#============================================================================================
# AJUSTAR Y COMPARAR DISTRIBUCIONES DISCRETAS GOF con chi-cuadrado
#============================================================================================
import numpy as np
import pandas as pd
from scipy import stats

# ------------------------------------------------------------------------------
# 1. FUNCI√ìN AUXILIAR (C√°lculo matem√°tico riguroso)
# ------------------------------------------------------------------------------
def calculate_chi2_robust(data, dist_name, params, n_params_est):
    """
    Realiza el test Chi-Cuadrado con agrupaci√≥n din√°mica (binning) 
    y normalizaci√≥n de probabilidades para cumplir criterios estad√≠sticos.
    """
    # Preparar conteos
    observed_counts = pd.Series(data).value_counts().sort_index()
    total_n = len(data)
    k_values = np.arange(observed_counts.index.min(), observed_counts.index.max() + 1)
    
    # Calcular PMF te√≥rica
    if dist_name == 'poisson':
        mu = params[0]
        probs = stats.poisson.pmf(k_values, mu)
    elif dist_name == 'geom':
        p, loc = params
        probs = stats.geom.pmf(k_values, p, loc=loc)
    elif dist_name == 'binom':
        n, p = params
        probs = stats.binom.pmf(k_values, n, p)
    elif dist_name == 'nbinom':
        n, p = params
        probs = stats.nbinom.pmf(k_values, n, p)
        
    expected_freqs = probs * total_n
    
    # Mapear observados
    obs_dict = observed_counts.to_dict()
    observed_freqs = np.array([obs_dict.get(k, 0) for k in k_values])
    
    # Agrupaci√≥n (Binning) - Regla de Cochran
    obs_grouped, exp_grouped = [], []
    curr_obs, curr_exp = 0, 0
    
    for o, e in zip(observed_freqs, expected_freqs):
        curr_obs += o
        curr_exp += e
        if curr_exp >= 5:
            obs_grouped.append(curr_obs)
            exp_grouped.append(curr_exp)
            curr_obs = 0
            curr_exp = 0
            
    if curr_exp > 0:
        if len(exp_grouped) > 0:
            exp_grouped[-1] += curr_exp
            obs_grouped[-1] += curr_obs
        else:
            exp_grouped.append(curr_exp)
            obs_grouped.append(curr_obs)

    # Normalizaci√≥n final
    obs_final = np.array(obs_grouped)
    exp_final = np.array(exp_grouped)
    if np.sum(exp_final) > 0:
        exp_final = exp_final * (np.sum(obs_final) / np.sum(exp_final))

    # Test
    n_bins = len(exp_final)
    dof = n_bins - 1 - n_params_est
    
    if dof <= 0:
        return np.nan, np.nan
        
    chi2_stat, p_val = stats.chisquare(f_obs=obs_final, f_exp=exp_final, ddof=n_params_est)
    
    return chi2_stat, p_val

# ------------------------------------------------------------------------------
# 2. FUNCI√ìN PRINCIPAL (Ajuste + Reporte Visual)
# ------------------------------------------------------------------------------
def best_fit_discrete(data):
    """
    Ajusta modelos discretos, calcula Chi2, imprime un reporte profesional
    y devuelve el DataFrame con los resultados.
    """
    # Limpieza de datos
    x = np.array(data)
    x = x[~np.isnan(x)]
    if len(x) == 0: 
        print("‚ùå Error: No hay datos v√°lidos.")
        return pd.DataFrame()
    
    # Estad√≠sticos muestrales
    mu = np.mean(x)
    var = np.var(x, ddof=1)
    min_val = np.min(x)
    if var == 0: var = 1e-6
    if mu == 0: mu = 1e-6
    
    results = []
    
    # --- AJUSTE DE MODELOS ---
    
    # 1. Poisson
    chi2, p = calculate_chi2_robust(x, 'poisson', [mu], 1)
    results.append({'Modelo': 'Poisson', 'Par√°metros_Txt': f'Œª={mu:.2f}', 'Chi2': chi2, 'P-Value': p, 'Params_Dict': {'mu': mu}})
    
    # 2. Geom√©trica
    if min_val == 0: p_geom = 1/(mu+1); loc_geom = -1; lbl = 'Geom (desde 0)'
    else: p_geom = 1/mu; loc_geom = 0; lbl = 'Geom (desde 1)'
    chi2, p = calculate_chi2_robust(x, 'geom', [p_geom, loc_geom], 1)
    results.append({'Modelo': lbl, 'Par√°metros_Txt': f'p={p_geom:.3f}', 'Chi2': chi2, 'P-Value': p, 'Params_Dict': {'p': p_geom, 'loc': loc_geom}})
    
    # 3. Binomial (Solo si Var < Mean)
    if var < mu:
        p_bin = 1 - (var/mu)
        n_bin = max(int(round(mu/p_bin)), int(np.max(x)))
        p_bin_adj = mu/n_bin
        chi2, p = calculate_chi2_robust(x, 'binom', [n_bin, p_bin_adj], 2)
        results.append({'Modelo': 'Binomial', 'Par√°metros_Txt': f'n={n_bin}, p={p_bin_adj:.2f}', 'Chi2': chi2, 'P-Value': p, 'Params_Dict': {'n': n_bin, 'p': p_bin_adj}})
    else:
        results.append({'Modelo': 'Binomial', 'Par√°metros_Txt': '-', 'Chi2': np.nan, 'P-Value': 0, 'Params_Dict': {}})

    # 4. Binomial Negativa (Solo si Var > Mean)
    if var > mu:
        p_nbin = mu/var
        n_val = (mu**2)/(var-mu)
        chi2, p = calculate_chi2_robust(x, 'nbinom', [n_val, p_nbin], 2)
        results.append({'Modelo': 'Binomial Negativa', 'Par√°metros_Txt': f'r={n_val:.2f}, p={p_nbin:.2f}', 'Chi2': chi2, 'P-Value': p, 'Params_Dict': {'n': n_val, 'p': p_nbin}})
    else:
        results.append({'Modelo': 'Binomial Negativa', 'Par√°metros_Txt': '-', 'Chi2': np.nan, 'P-Value': 0, 'Params_Dict': {}})

    # --- PROCESAMIENTO FINAL ---
    df = pd.DataFrame(results).dropna(subset=['Chi2'])
    
    if df.empty:
        print("‚ö†Ô∏è No se pudo ajustar ning√∫n modelo v√°lido (¬øpocos datos?).")
        return df

    # Crear columnas visuales y ordenar
    df['Decision'] = df['P-Value'].apply(lambda val: '‚úÖ Aceptable' if val > 0.05 else '‚ùå Rechazado')
    df = df.sort_values(by='P-Value', ascending=False).reset_index(drop=True)

    # --- IMPRESI√ìN DEL REPORTE "BONITO" ---
    print("\n" + "‚ïê"*75)
    print("üìä  RESULTADOS DEL AJUSTE DE DISTRIBUCIONES (CHI-CUADRADO)")
    print("‚ïê"*75)
    
    # Imprimir tabla limpia
    cols_visuales = ['Modelo', 'Par√°metros_Txt', 'Chi2', 'P-Value', 'Decision']
    # Formato de pandas para que se vea alineado
    print(df[cols_visuales].to_string(index=False, formatters={
        'Chi2': '{:,.4f}'.format,
        'P-Value': '{:,.4f}'.format
    }))
    print("‚îÄ"*75)

    # --- CONCLUSI√ìN AUTOM√ÅTICA ---
    best_row = df.iloc[0]
    best_model = best_row['Modelo']
    best_p = best_row['P-Value']
    best_params = best_row['Params_Dict']

    print(f"\nüèÜ  MEJOR MODELO SELECCIONADO: \033[1m{best_model}\033[0m")
    
    if best_p > 0.05:
        print(f"    ‚úÖ Ajuste estad√≠sticamente v√°lido (P-Value: {best_p:.4f} > 0.05).")
        print("       No hay evidencia suficiente para rechazar que los datos sigan esta distribuci√≥n.")
    else:
        print(f"    ‚ö†Ô∏è  Ajuste pobre (P-Value: {best_p:.4f} < 0.05).")
        print("       El modelo es la mejor opci√≥n disponible, pero estad√≠sticamente no es perfecto.")

    print(f"\n‚öôÔ∏è  PAR√ÅMETROS T√âCNICOS (Diccionario):")
    print(f"    {best_params}")
    print("‚ïê"*75 + "\n")
    
    return df
  
#============================================================================================
# AJUSTE Y COMPARACI√ìN DE DISTRIBUCIONES CONTINUAS GOF
#============================================================================================

def gof_continuous(data):
    """
    Ajusta distribuciones continuas (MOM) y genera un reporte visual profesional.
    Devuelve un DataFrame con par√°metros accesibles en 'Params_Dict'.
    """
    
    # 1. Preparaci√≥n de datos
    x = np.array(data)
    x = x[~np.isnan(x)]
    if len(x) == 0:
        print("‚ùå Error: No hay datos v√°lidos.")
        return pd.DataFrame()

    # Estad√≠sticos b√°sicos
    mu = np.mean(x)
    var = np.var(x, ddof=1)
    std = np.std(x, ddof=1)
    x_min = np.min(x)
    x_max = np.max(x)
    
    results = []

    # ==============================================================================
    # 1. UNIFORME
    # ==============================================================================
    range_uni = np.sqrt(12 * var)
    uni_a = mu - (range_uni / 2)
    uni_scale = range_uni
    
    d, p = stats.kstest(x, 'uniform', args=(uni_a, uni_scale))
    results.append({
        'Distribuci√≥n': 'Uniforme',
        'Par√°metros_Txt': f'Min={uni_a:.2f}, Range={uni_scale:.2f}',
        'Params_Dict': {'loc': uni_a, 'scale': uni_scale},
        'KS Stat': d, 'P-Value': p
    })

    # ==============================================================================
    # 2. EXPONENCIAL
    # ==============================================================================
    exp_scale = mu
    d, p = stats.kstest(x, 'expon', args=(0, exp_scale))
    results.append({
        'Distribuci√≥n': 'Exponencial',
        'Par√°metros_Txt': f'Scale={exp_scale:.2f}',
        'Params_Dict': {'loc': 0, 'scale': exp_scale},
        'KS Stat': d, 'P-Value': p
    })

    # ==============================================================================
    # 3. NORMAL
    # ==============================================================================
    d, p = stats.kstest(x, 'norm', args=(mu, std))
    results.append({
        'Distribuci√≥n': 'Normal',
        'Par√°metros_Txt': f'Mu={mu:.2f}, Std={std:.2f}',
        'Params_Dict': {'loc': mu, 'scale': std},
        'KS Stat': d, 'P-Value': p
    })

    # ==============================================================================
    # 4. GAMMA
    # ==============================================================================
    if var > 0 and mu != 0:
        gam_scale = var / mu
        gam_a = (mu ** 2) / var
        d, p = stats.kstest(x, 'gamma', args=(gam_a, 0, gam_scale))
        results.append({
            'Distribuci√≥n': 'Gamma',
            'Par√°metros_Txt': f'Alpha={gam_a:.2f}, Beta={gam_scale:.2f}',
            'Params_Dict': {'a': gam_a, 'loc': 0, 'scale': gam_scale},
            'KS Stat': d, 'P-Value': p
        })

    # ==============================================================================
    # 5. ERLANG (Gamma con shape entero)
    # ==============================================================================
    if var > 0 and mu != 0:
        erl_k = max(1, round((mu ** 2) / var))
        erl_scale = mu / erl_k
        d, p = stats.kstest(x, 'gamma', args=(erl_k, 0, erl_scale))
        results.append({
            'Distribuci√≥n': 'Erlang',
            'Par√°metros_Txt': f'k={int(erl_k)}, Beta={erl_scale:.2f}',
            'Params_Dict': {'a': erl_k, 'loc': 0, 'scale': erl_scale},
            'KS Stat': d, 'P-Value': p
        })

    # ==============================================================================
    # 6. TRIANGULAR
    # ==============================================================================
    tri_loc = x_min
    tri_scale = x_max - x_min
    mode_est = 3 * mu - x_min - x_max
    mode_est = max(x_min, min(x_max, mode_est)) # Clamp
    
    if tri_scale > 0:
        tri_c = (mode_est - tri_loc) / tri_scale
        d, p = stats.kstest(x, 'triang', args=(tri_c, tri_loc, tri_scale))
        results.append({
            'Distribuci√≥n': 'Triangular',
            'Par√°metros_Txt': f'c={tri_c:.2f}, Loc={tri_loc:.2f}, Scale={tri_scale:.2f}',
            'Params_Dict': {'c': tri_c, 'loc': tri_loc, 'scale': tri_scale},
            'KS Stat': d, 'P-Value': p
        })

    # ==============================================================================
    # 7. WEIBULL
    # ==============================================================================
    if mu > 0 and std > 0:
        cv_sq = (std / mu) ** 2
        def weibull_eq(k):
            if k <= 0: return 100.0
            return (gamma(1 + 2/k) / (gamma(1 + 1/k)**2)) - 1 - cv_sq

        try:
            wei_k = optimize.fsolve(weibull_eq, 1.0)[0]
        except:
            wei_k = 1.0
        
        if wei_k > 0:
            wei_scale = mu / gamma(1 + 1/wei_k)
            d, p = stats.kstest(x, 'weibull_min', args=(wei_k, 0, wei_scale))
            results.append({
                'Distribuci√≥n': 'Weibull',
                'Par√°metros_Txt': f'Shape={wei_k:.2f}, Scale={wei_scale:.2f}',
                'Params_Dict': {'c': wei_k, 'loc': 0, 'scale': wei_scale},
                'KS Stat': d, 'P-Value': p
            })

    # ==============================================================================
    # 8. LOG-NORMAL
    # ==============================================================================
    if min_val := np.min(x) > 0: # Solo si todos son positivos
        phi = np.sqrt(var + mu**2)
        mu_log = np.log(mu**2 / phi)
        sigma_log = np.sqrt(np.log(phi**2 / mu**2))
        scale_log = np.exp(mu_log)
        
        d, p = stats.kstest(x, 'lognorm', args=(sigma_log, 0, scale_log))
        results.append({
            'Distribuci√≥n': 'Log-Normal',
            'Par√°metros_Txt': f's={sigma_log:.2f}, Scale={scale_log:.2f}',
            'Params_Dict': {'s': sigma_log, 'loc': 0, 'scale': scale_log},
            'KS Stat': d, 'P-Value': p
        })

    # --- PROCESAMIENTO FINAL ---
    df = pd.DataFrame(results)
    if df.empty: return df

    df['Decision'] = df['P-Value'].apply(lambda val: '‚úÖ Aceptable' if val > 0.05 else '‚ùå Rechazado')
    df = df.sort_values(by='P-Value', ascending=False).reset_index(drop=True)

    # --- REPORTE VISUAL ---
    print("\n" + "‚ïê"*80)
    print("üìä  RESULTADOS DEL AJUSTE (DISTRIBUCIONES CONTINUAS - KS TEST)")
    print("‚ïê"*80)
    
    cols_show = ['Distribuci√≥n', 'Par√°metros_Txt', 'KS Stat', 'P-Value', 'Decision']
    print(df[cols_show].to_string(index=False, formatters={
        'KS Stat': '{:.4f}'.format,
        'P-Value': '{:.4f}'.format
    }))
    print("‚îÄ"*80)

    # Ganador
    best = df.iloc[0]
    print(f"\nüèÜ  MEJOR AJUSTE: \033[1m{best['Distribuci√≥n']}\033[0m")
    print(f"    P-Value: {best['P-Value']:.4f}")
    if best['P-Value'] > 0.05:
        print("    ‚úÖ No hay evidencia para rechazar esta distribuci√≥n.")
    else:
        print("    ‚ö†Ô∏è Precauci√≥n: El ajuste no es ideal (P-Value < 0.05).")
    
    print(f"\n‚öôÔ∏è  PAR√ÅMETROS T√âCNICOS (Para Scipy):")
    print(f"    {best['Params_Dict']}")
    print("‚ïê"*80 + "\n")
    
    # acceder al ganador  
    # ganador = df_ajuste.iloc[0]
    # modelo = ganador['Distribuci√≥n']
    # params = ganador['Params_Dict']
    
    return df
  
#============================================================================================
# CMTD
#============================================================================================

# Calcular la matriz de transici√≥n de n pasos
def cmtd_matrix_n(mc, n):
  """
  Funci√≥n para obtener la matriz de transici√≥n de n pasos
  dada un proceso definido con MarkovChain()

  Par√°metros de entrada:
    - mc: proceso definido con MarkovChain()
    - n: n√∫mero de saltos.

  Par√°metros de salida:
    - p_n: matriz de transici√≥n de n pasos.
  """
  import pydtmc
  mtn  = pydtmc.MarkovChain(np.linalg.matrix_power(mc.p, n), mc.states)
  return pd.DataFrame(mtn.p,columns=mc.states,index=mc.states)
#--------------------------------------------------------------------------------------
# Matriz de tiempos de ocupaci√≥n
def mat_ocupacion_proceso(mc, n):
  """
  Funci√≥n para obtener la matriz de ocupaci√≥n asocida al proceso mc en n transiciones

  Par√°metros de entrada:
  - mc: proceso
  - n: n√∫mero de transiciones

  Par√°metros de salida:
  - mocupa: matriz de ocupacion
  """
  mocupa = np.zeros((len(mc.states), len(mc.states)))
  for i in range(n+1):
    mocupa += np.linalg.matrix_power(mc.p, i)

  return mocupa
  import pydtmc
  mtn  = pydtmc.MarkovChain(np.linalg.matrix_power(mc.p, n), mc.states)
  return pd.DataFrame(mtn.p,columns=mc.states,index=mc.states)
